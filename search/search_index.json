{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-q","title":"KIT-Q","text":"<p>Ian Faray </p> <p>Vitor Tanaka</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 17/03/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li>[In progress ] Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1 - Computa\u00e7\u00e3o em Nuvem","text":"<p>Membros: Ian Faray e Vitor Tanaka</p>"},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Este roteiro tem como objetivo demonstrar o processo de cria\u00e7\u00e3o de uma infraestrutura de Nuvem Bare-metal, em que introduziram-se conceitos b\u00e1sicos sobre gerenciamento de hardware e rede de computadores. Utilizamos um ambiente composto por cinco m\u00e1quinas, onde implementamos:</p> <ul> <li> <p>Um servidor PostgreSQL para gerenciamento de banco de dados.</p> </li> <li> <p>Duas inst\u00e2ncias de uma aplica\u00e7\u00e3o Django, distribu\u00eddas em servidores distintos.</p> </li> <li> <p>Um servidor Nginx configurado como balanceador de carga, distribuindo as requisi\u00e7\u00f5es entre as aplica\u00e7\u00f5es Django.</p> </li> </ul>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":""},{"location":"roteiro1/main/#infraestrutura","title":"Infraestrutura","text":"<p>Inicialmente, criamos a infraestrutura de Nuvem Bare-metal:</p> <ul> <li> <p>Instalamos o Ubuntu Server 22.04 LTS na NUC principal, sendo nossa main, utilizando um pendrive boot\u00e1vel.</p> </li> <li> <p>Configuramos o MAAS para gerenciar as m\u00e1quinas, sendo acessado via autentica\u00e7\u00e3o SSH.</p> </li> <li> <p>Adicionamos as 5 m\u00e1quinas dispon\u00edveis (server1 a server5) ao MAAS, configurando-as para inicializa\u00e7\u00e3o via PXE.</p> </li> <li> <p>Criamos uma OVS Bridge para conectar as interfaces de rede, garantindo a comunica\u00e7\u00e3o entre as m\u00e1quinas.</p> </li> <li> <p>Realizamos um NAT para permitir o acesso externo a partir da rede Insper, conectando ao server main pela porta 22.</p> </li> </ul>"},{"location":"roteiro1/main/#aplicacao","title":"Aplica\u00e7\u00e3o","text":"<p>Primeiramente, realizamos o deploy do Ubuntu no server1 pelo MAAS, em que, em seguida, instalamos o PostgreSQL.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p> Tarefa 1.1 Print Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional. </p> <p> Tarefa 1.2 Acessivel na pr\u00f3pria maquina na qual ele foi implantado. Tarefa 1.3 Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN</p> <p> Tarefa 1.4 Em qual porta este servi\u00e7o est\u00e1 funcionando</p>"},{"location":"roteiro1/main/#parte-ii-aplicacao-django","title":"Parte II: Aplica\u00e7\u00e3o Django","text":"<p>No segundo servidor, assim como no primeiro, realizamos o deploy do Ubuntu, por\u00e9m neste caso instalamos uma aplica\u00e7\u00e3o Django j\u00e1 desenvolvida.   </p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p> Tarefa 2.1 Dashboard do MAAS com as m\u00e1quinas</p> <p> Tarefa 2.2 aba images, com as imagens sincronizadas.</p> <p> Tarefa 2.3-1</p> <p> Tarefa 2.3-2</p> <p> Tarefa 2.3-3</p> <p> Tarefa 2.3-4</p> <p> Tarefa 2.3-5 Da Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK</p>"},{"location":"roteiro1/main/#utilizando-o-ansible-deploy-automatizado-de-aplicacao","title":"Utilizando o Ansible - deploy automatizado de aplica\u00e7\u00e3o","text":""},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<p> Tarefa 3.1  print da tela do Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs.</p> <p> Tarefa 3.2 print da aplicacao Django, provando que voce est\u00e1 conectado ao server</p> <p>tarefa 3.3:  A implementa\u00e7\u00e3o manual do banco de dados no servidor 1 foi realizada via terminal, come\u00e7ando com a instala\u00e7\u00e3o do PostgreSQL. Em seguida, foi criado um superusu\u00e1rio e uma base de dados, al\u00e9m de configurar o banco para aceitar conex\u00f5es de qualquer m\u00e1quina dentro da nossa sub-rede. Para a aplica\u00e7\u00e3o Django no servidor 2, clonamos um reposit\u00f3rio que j\u00e1 continha o c\u00f3digo da aplica\u00e7\u00e3o e utilizamos um script (install.sh). Esse script instalava automaticamente todas as depend\u00eancias necess\u00e1rias, realizava a migra\u00e7\u00e3o do banco de dados e configurava outro script para garantir que o servidor fosse iniciado automaticamente ap\u00f3s um reboot. A aplica\u00e7\u00e3o foi configurada para rodar na porta 8080, e tamb\u00e9m foi criado um superusu\u00e1rio para acesso ao Django Admin. Para conseguirmos testar a aplica\u00e7\u00e3o Django rodando no nosso computador, configuramos um t\u00fanel SSH redirecionando a porta 8080 do servidor remoto para a porta 8001 local.</p>"},{"location":"roteiro1/main/#ansible","title":"Ansible","text":"<p>Utilizamos o Ansible para fazermos deploy da aplica\u00e7\u00e3o Django. No nosso caso, escolhemos o server 4.</p>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<p> Tarefa 4.1 print da tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs.</p> <p> *Tarefa 4.2 print da aplicacao Django, provando que voce est\u00e1 conectado ao server2 *</p> <p> Tarefa 4.3 print da aplicacao Django, provando que voce est\u00e1 conectado ao server3(4) no nosso caso</p> <p>Tarefa 4.4:A grande diferen\u00e7a \u00e9 que o Ansible permite automatizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o do Django, enquanto a instala\u00e7\u00e3o manual exige que cada etapa seja realizada individualmente. Com o Ansible, utilizamos um playbook que cont\u00e9m todos os scripts necess\u00e1rios para definir e executar as instru\u00e7\u00f5es de instala\u00e7\u00e3o e configura\u00e7\u00e3o do Django. Na instala\u00e7\u00e3o manual, \u00e9 preciso realizar cada passo manualmente, como clonar o reposit\u00f3rio e ajustar configura\u00e7\u00f5es, o que pode ser mais demorado e propenso a erros. J\u00e1 com o Ansible, h\u00e1 um script padronizado que n\u00e3o s\u00f3 agiliza o processo, mas tamb\u00e9m facilita para escalonar o projeto caso necess\u00e1rio</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<p> Tarefa 5.1 print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.</p> <p> Tarefa 5.2-1</p> <p> Tarefa 5.2-2prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3 (no nosso caso, server4).</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Apesar da pr\u00e1tica ter sido um sucesso, encontramos diversos obst\u00e1culos e dificuldades no processo at\u00e9 atingirmos o resultado final. O primeiro contato com o MAAS e a configura\u00e7\u00e3o da infraestrutura foi desafiador, sendo necess\u00e1rio pesquisar e consultar os professores e assistentes, que nos orientaram e auxiliaram para superar as dificuldades. Al\u00e9m disso, imprevistos ocorreram, at\u00e9 inesperados pelos professores, como por exemplo, as imagens do Ubuntu que instalamos no MAAS sofreram uma atualiza\u00e7\u00e3o e ficaram incompat\u00edveis com o firmware das m\u00e1quinas, o que nos obrigou a voltar alguns passos para continuar o roteiro. Apesar disso, esses incidentes s\u00e3o importantes para aprendermos a lidar com problemas reais e a buscar solu\u00e7\u00f5es em grupo, o que contribui para o nosso desenvolvimento profissional e conhecimento t\u00e9cnico.</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro permitiu consolidar conhecimentos sobre redes, sub-redes e gerenciamento de infraestrutura em nuvem. Al\u00e9m disso, exploramos ferramentas como MAAS, Ansible e Nginx, que facilitam a implementa\u00e7\u00e3o e o gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas. O balanceamento de carga e a alta disponibilidade foram alcan\u00e7ados com sucesso, demonstrando a efic\u00e1cia da infraestrutura criada.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2: Utilizando a Infraestrutura Bare Metal com o Juju","text":""},{"location":"roteiro2/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este roteiro tem como objetivo demonstrar o uso do Juju como orquestrador de aplica\u00e7\u00f5es distribu\u00eddas, integrado ao MAAS para provisionamento autom\u00e1tico de m\u00e1quinas bare-metal. Como aplica\u00e7\u00e3o de exemplo, implementamos o Grafana e o Prometheus, dois servi\u00e7os populares para monitoramento e visualiza\u00e7\u00e3o de dados.</p>"},{"location":"roteiro2/main/#configuracao-do-ambiente","title":"Configura\u00e7\u00e3o do Ambiente","text":""},{"location":"roteiro2/main/#instalacao-do-juju","title":"Instala\u00e7\u00e3o do Juju","text":"<ul> <li> <p>Instalamos o Juju na m\u00e1quina main.</p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre> </li> </ul>"},{"location":"roteiro2/main/#integracao-com-o-maas","title":"Integra\u00e7\u00e3o com o MAAS","text":"<ul> <li> <p>Criamos um arquivo de configura\u00e7\u00e3o (maas-cloud.yaml) para adicionar o cluster MAAS ao Juju e um arquivo de credenciais (maas-creds.yaml) para autentica\u00e7\u00e3o.</p> <p><pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre></p> </li> </ul>"},{"location":"roteiro2/main/#criacao-do-controlador","title":"Cria\u00e7\u00e3o do Controlador","text":"<ul> <li>Criamos o controlador Juju no servidor server1 utilizando a tag juju.  </li> <li>O controlador foi configurado para gerenciar toda a infraestrutura.</li> </ul> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre>"},{"location":"roteiro2/main/#deploy-das-aplicacoes","title":"Deploy das Aplica\u00e7\u00f5es","text":""},{"location":"roteiro2/main/#instalacao-do-dashboard-do-juju","title":"Instala\u00e7\u00e3o do Dashboard do Juju","text":"<ul> <li> <p>Instalamos o Dashboard do Juju </p> <p><pre><code>$ juju switch controller\n</code></pre> <pre><code>$ juju deploy juju-dashboard dashboard\n</code></pre> <pre><code>$ juju integrate dashboard controller\n</code></pre> <pre><code>$ juju expose dashboard\n</code></pre> <pre><code>$ juju dashboard\n</code></pre></p> </li> </ul>"},{"location":"roteiro2/main/#deploy-do-grafana-e-prometheus","title":"Deploy do Grafana e Prometheus","text":"<ul> <li> <p>Instalamos o Grafana e Prometheus e realizamos o deploy local dos charms respectivos utilizando o Juju.  </p> <p><pre><code>$ mkdir -p /home/cloud/charms\n</code></pre> <pre><code>$ cd /home/cloud/charms\n</code></pre> <pre><code>$ juju download grafana\n</code></pre> <pre><code>$ juju download prometheus2\n</code></pre> - Deploy dos charms: <pre><code>$ juju deploy (grafana e prometheus)\n</code></pre></p> </li> </ul>"},{"location":"roteiro2/main/#integracao-entre-grafana-e-prometheus","title":"Integra\u00e7\u00e3o entre Grafana e Prometheus","text":"<ul> <li> <p>Integramos o Grafana com o Prometheus.  </p> <pre><code>$ juju integrate grafana prometheus2\n</code></pre> </li> </ul>"},{"location":"roteiro2/main/#atividades","title":"Atividades","text":"<p> 1 - Print da tela do Dashboard do MAAS com as Maquinas e seus respectivos IPs </p> <p> 2 - Print de tela do comando \"juju status\" depois que o Grafana estiver \"active\"</p> <p> 3 - Print da tela do Dashboard do Grafana com o Prometheus aparecendo como source</p> <p> 4 - Print que voc\u00ea est\u00e1 conseguindo acessar o Dashboard a partir da rede do Insper</p> <p> 5 - Print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU</p>"},{"location":"roteiro2/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Apesar de curto, encontramos algumas dificuldades ao longo da realiza\u00e7\u00e3o do roteiro. Como foi um primeiro contato com o Juju, \u00e9 natural que tenha sido um desafio a instala\u00e7\u00e3o e configura\u00e7\u00e3o correta do mesmo, envolvendo leituras de documenta\u00e7\u00f5es e entendimento de seu funcionamento e prop\u00f3sito. Por exemplo, ao realizarmos o deploy do charm do Grafana, tivemos que for\u00e7ar um versionamento correto do Ubuntu (20.04), pois a vers\u00e3o instalada pelo padr\u00e3o do Juju (22.04) n\u00e3o era compat\u00edvel com o charm. Os problemas enfrentados geram um engajamento maior com o aprendizado da ferramenta e nos obrigam a buscar entender o funcionamento do Juju e os conceitos por tr\u00e1s dele.</p>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro nos proporcionou um aprendizado significativo sobre o uso do Juju como orquestrador de aplica\u00e7\u00f5es distribu\u00eddas, al\u00e9m de nos familiarizar ainda mais com o MAAS para provisionamento autom\u00e1tico de m\u00e1quinas bare-metal. A integra\u00e7\u00e3o entre o Grafana e o Prometheus demonstrou a efic\u00e1cia do Juju na gest\u00e3o de servi\u00e7os complexos, facilitando a implementa\u00e7\u00e3o e o monitoramento de aplica\u00e7\u00f5es em ambientes bare-metal. A experi\u00eancia adquirida neste roteiro ser\u00e1 valiosa para futuras implementa\u00e7\u00f5es e projetos que envolvam orquestra\u00e7\u00e3o e automa\u00e7\u00e3o de infraestrutura.</p>"},{"location":"roteiro3/main/","title":"Roteiro 3: Private cloud com Openstack e Juju","text":""},{"location":"roteiro3/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este roteiro tem como objetivo a implementa\u00e7\u00e3o de um ambiente de nuvem privada, entendendo os conceitos b\u00e1sicos e aplicando na pr\u00e1tica com o uso do OpenStack e do Juju. O OpenStack \u00e9 uma plataforma de computa\u00e7\u00e3o em nuvem de c\u00f3digo aberto que permite a cria\u00e7\u00e3o e gerenciamento de infraestrutura como servi\u00e7o (IaaS). O Juju, j\u00e1 utilizado anteriormente, \u00e9 uma ferramenta de orquestra\u00e7\u00e3o que facilita o gerenciamento de servi\u00e7os e aplica\u00e7\u00f5es em ambientes distribu\u00eddos.</p>"},{"location":"roteiro3/main/#configuracao-do-ambiente-infra","title":"Configura\u00e7\u00e3o do Ambiente - Infra","text":""},{"location":"roteiro3/main/#implantacao-do-openstack","title":"Implanta\u00e7\u00e3o do OpenStack","text":"<ul> <li> <p>Primeiramente, no MAAS, adicionamos as seguintes tags \u00e0s m\u00e1quinas:</p> <ul> <li>controller - node1</li> <li>reserva - node2 (ser\u00e1 usado posteriormente)</li> <li>compute - node3, node4 e node5</li> </ul> </li> <li> <p>Com isso, verificamos no MAAS se o br-ex estava configurado corretamente em todos os n\u00f3s, para assim, o OpenStack conseguir se comunicar com a rede externa.</p> </li> <li> <p>Ap\u00f3s as configura\u00e7\u00f5es iniciais, implementamos o deploy do juju controller no node1, para fazermos as instala\u00e7\u00f5es necess\u00e1rias em todos os n\u00f3s.</p> </li> <li> <p>Por fim, com os comandos juju deploy e juju integrate e com arquivos de configura\u00e7\u00e3o .yaml, instalamos todos os programas requisitados no roteiro, sempre verificando pelo juju status as m\u00e1quinas e instala\u00e7\u00f5es.</p> <ul> <li>Ceph OSD</li> <li>Nova Compute</li> <li>MySQL InnoDB Cluster</li> <li>Vault</li> <li>Neutron Networking</li> <li>Keystone</li> <li>RabbitMQ</li> <li>Nova Cloud Controller</li> <li>Placement</li> <li>Horizon - OpenStack Dashboard</li> <li>Glance</li> <li>Ceph Monitor</li> <li>Cinder</li> <li>Ceph RADOS Gateway</li> <li>Ceph-OSD Integration para as m\u00e1quinas e aplica\u00e7\u00f5es estarem active</li> </ul> </li> </ul>"},{"location":"roteiro3/main/#configurando-o-openstack-setup","title":"Configurando o OpenStack - Setup","text":"<ul> <li>Pr\u00f3ximo passo foi configurar o OpenStack, configurando os servi\u00e7os que controlam VMs (Nova), os volumes de disco (Cinder) e a estrutura de rede virtual (Neutron). Para isso, a realiza\u00e7\u00e3o foi separada nos seguintes passos: </li> </ul>"},{"location":"roteiro3/main/#passo-1-autenticacao","title":"Passo 1 - Autentica\u00e7\u00e3o","text":"<ul> <li>Na main, criamos o arquivo openrc, que cont\u00e9m as credenciais de acesso ao OpenStack.</li> </ul> <p><pre><code> source openrc \n</code></pre> <pre><code>env | grep OS_\n</code></pre>  - Com isso, conseguimos o usu\u00e1rio e senha para acessar o Horizon, o Dashboard do OpenStack.</p>"},{"location":"roteiro3/main/#passo-2-horizon","title":"Passo 2 - Horizon","text":"<ul> <li>Com as credenciais prontas, acessamos o Dashboard do Openstack, chamado de Horizon. Para isso, realizamos um NAT no roteador para o Horizon na porta interna 80 e porta externa 8080, acessando a partir do 10.103.1.26:8080/horizon. O Domain name utilizado no acesso foi admin_domain.</li> </ul>"},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":"<p> 1 - Print do status do Juju </p> <p> 2 - Print do Dashboard do MAAS com as m\u00e1quinas</p> <p> 3 - Print da aba compute overview no OpenStack Dashboard</p> <p> 4 - Print da aba compute instances no OpenStack Dashboard</p> <p> 5 - Print da aba network topology no Openstack Dashboard</p>"},{"location":"roteiro3/main/#passo-3-imagens-e-flavors","title":"Passo 3 - Imagens e Flavors","text":"<ul> <li> <p>Utilizando a documenta\u00e7\u00e3o oficial do OpenStack, nessa etapa criamos imagens e flavors para nossa nuvem. Imagem se refere ao sistema operacional a ser bootado nas m\u00e1quinas virtuais a serem implementadas. J\u00e1 os flavors determinam os specs de hardware da VM, por exemplo: mem\u00f3ria RAM, armazenamento, CPU.</p> </li> <li> <p>Para isso, primeiramente instalamos o client do OpenStack na main.  <pre><code>sudo snap install openstackclients\n</code></pre></p> </li> <li>Em seguida, carregamos as credenciais, verificamos os servi\u00e7os dispon\u00edveis e fizemos pequenos ajustes na rede.  <pre><code>source openrc\n</code></pre> <pre><code>openstack service list\n</code></pre> <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.16.0.1\"\n</code></pre></li> <li>Logo ap\u00f3s, importamos a imagem do Ubuntu Jammy e a chamamos de jammy-amd64  <pre><code>mkdir ~/cloud-images\n\nwget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\\n  -O ~/cloud-images/jammy-amd64.img\n</code></pre> <pre><code>openstack image create --public --container-format bare \\\n  --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\\n  jammy-amd64\n</code></pre></li> <li>Por fim, criamos 4 flavors:<ul> <li><code>m1.tiny: 1 vCPUs, 1Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 1024 --disk 20 --vcpus 1 m1.tiny\n</code></pre></li> <li><code>m1.small: 1 vCPUs, 2Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 2048 --disk 20 --vcpus 1 m1.small\n</code></pre></li> <li><code>m1.medium: 2 vCPUs, 4Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 4096 --disk 20 --vcpus 2 m1.medium\n</code></pre></li> <li><code>m1.large: 4 vCPUs, 8Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 8192 --disk 20 --vcpus 4 m1.large\n</code></pre></li> </ul> </li> </ul>"},{"location":"roteiro3/main/#passo-4-rede-externa","title":"Passo 4 - Rede Externa","text":"<ul> <li>Nessa etapa, configuramos a rede externa da nuvem, com uma faixa entre <code>172.16.7.0</code> e <code>172.16.8.255</code> <pre><code>openstack network create --external --share \\\n  --provider-network-type flat --provider-physical-network physnet1 \\\n  ext_net\n</code></pre></li> <li>Subnet  <pre><code>openstack subnet create --network ext_net --no-dhcp \\\n  --gateway 172.16.0.1 --subnet-range 172.16.0.0/2 \\\n  --allocation-pool start=172.16.7.0,end=172.16.8.255 \\\n  ext_subnet\n</code></pre></li> </ul>"},{"location":"roteiro3/main/#passo-5-rede-interna-e-roteador","title":"Passo 5 - Rede Interna  e  Roteador","text":"<ul> <li>Nessa etapa, configuramos a rede interna da nuvem, com a subnet <code>192.169.0.0/24</code>, e o roteador.  <pre><code>openstack network create --internal user1_net\n\nopenstack subnet create --network user1_net \\\n  --subnet-range 192.169.0.0/24 \\\n  --allocation-pool start=192.169.0.10,end=192.169.0.90 \\\n  user1_subnet\n</code></pre> <pre><code>openstack router create user1_router\nopenstack router add subnet user1_router user1_subnet\nopenstack router set user1_router --external-gateway ext_net\n</code></pre></li> </ul>"},{"location":"roteiro3/main/#passo-6-conexao","title":"Passo 6 - Conex\u00e3o","text":"<ul> <li>Primeiramente, importamos a chave p\u00fablica j\u00e1 gerada anteriormente para o MAAS, para que possamos acessar as VMs via SSH.   <pre><code>openstack keypair create --public-key ~/.ssh/id_rsa.pub user1\n</code></pre></li> <li>Em seguida, usando o Horizon, adicionamos a libera\u00e7\u00e3o do SSH e ALL ICMP no Security Group, para que possamos acessar as VMs e fazer ping. </li> </ul>"},{"location":"roteiro3/main/#passo-7-instancia","title":"Passo 7 - Inst\u00e2ncia","text":"<ul> <li>Com tudo configurado, criamos uma inst\u00e2ncia com o flavor m1.tiny.   <pre><code>openstack server create --flavor m1.tiny --image jammy-amd64 \\\n  --key-name user1 --network user1_net --security-group default \\\n  jammy-1\n</code></pre></li> <li>Ap\u00f3s isso, alocamos um floating IP para a inst\u00e2ncia.   <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip jammy-1 $FLOATING_IP\n</code></pre></li> <li>Com isso, conseguimos acessar a inst\u00e2ncia via SSH, usando o IP flutuante.   <pre><code>ssh ubuntu@&lt;floating_ip&gt;\n</code></pre></li> </ul>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":"<p> 1 - Print do Dashboard do MAAS com as m\u00e1quinas</p> <p> 2 - Print da aba compute overview no OpenStack Dashboard</p> <p> 3 - Print da aba compute instances no OpenStack Dashboard</p> <p> 4 - Print da aba network topology no Openstack Dashboard</p> <ul> <li>As diferen\u00e7as encontradas entre os prints da tarefa 1 e tarefa 2 s\u00e3o simplesmente o resultado da execu\u00e7\u00e3o dos comandos de configura\u00e7\u00e3o do OpenStack. O dashboard do MAAS n\u00e3o mudou, j\u00e1 que tivemos problemas com o server3 e, portanto, adicionamos o n\u00f3 reserva (node2) na parte de instala\u00e7\u00e3o antes da etapa de escalonamento de n\u00f3s. A aba compute overview nos mostra uma vis\u00e3o geral, com todos os recursos adicionados no setup. J\u00e1 a aba compute instances mostra a inst\u00e2ncia jammy1 criada. Por fim, a aba network topology mostra a rede interna e externa criadas, com o roteador e as subnets, todas criadas no setup.</li> <li>Todos os recursos foram criados pela CLI do OpenStack, em que instalamos o openstackclient no come\u00e7o da etapa.</li> </ul>"},{"location":"roteiro3/main/#escalando-os-nos","title":"Escalando os n\u00f3s","text":"<ul> <li>Como dito anteriormente, o server3 apresentou problemas, ent\u00e3o realizamos esta etapa antes do setup com os seguintes comandos:   <pre><code>juju add-unit nova-compute\n</code></pre> <pre><code>juju add-unit --to &lt;id do node2&gt; ceph-osd\n</code></pre></li> </ul>"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":"<pre><code>graph TD\n    A[Seu computador] -- 10.0.0.0/8 --&gt; B[Roteador Insper]\n    B -- 172.16.0.0/20 --&gt; C[OpenStack Controller main]\n    C -- T\u00fanel SSH --&gt; D[Load Balancer]\n    D -- 192.169.0.28 --&gt; E[API 1]\n    D -- 192.169.0.54 --&gt; F[API 2]\n    E -- 192.169.0.85 --&gt; G[Database]\n    F -- 192.169.0.85 --&gt; G[Database]</code></pre>"},{"location":"roteiro3/main/#app","title":"App","text":""},{"location":"roteiro3/main/#uso-da-infraestrutura","title":"Uso da infraestrutura","text":"<ul> <li> <p>Ultima etapa do roteiro foi integrar a API criada na primeira etapa do projeto da disciplina com a nuvem privada criada. </p> </li> <li> <p>Para isso, foi necess\u00e1rio levantar as aplica\u00e7\u00f5es em 4 inst\u00e2ncias do OpenStack:</p> <ul> <li>2 inst\u00e2ncias com a API do projeto, implementada em FastAPI - m1.tiny</li> <li>1 inst\u00e2ncia com o banco de dados, implementado em MySQL - m1.small</li> <li>1 inst\u00e2ncia com LoadBalancer, implementado em Nginx - m1.tiny</li> </ul> <p></p> </li> <li> <p>Para isso, utilizamos o mesmo procedimento da etapa anterior, criando as inst\u00e2ncias com os flavors m1.tiny e m1.small alocando os IPs flutuantes.   <pre><code>openstack server create --flavor m1.tiny --image jammy-amd64 \\\n  --key-name user1 --network user1_net --security-group default \\\n  API_1\n</code></pre> <pre><code>openstack server create --flavor m1.tiny --image jammy-amd64 \\\n  --key-name user1 --network user1_net --security-group default \\\n  API_2\n</code></pre> <pre><code>openstack server create --flavor m1.small --image jammy-amd64 \\\n  --key-name user1 --network user1_net --security-group default \\\n  DATABASE\n</code></pre> <pre><code>openstack server create --flavor m1.tiny --image jammy-amd64 \\\n  --key-name user1 --network user1_net --security-group default \\\n  LOADBALANCER\n</code></pre></p> </li> <li>Ap\u00f3s isso, alocamos os IPs flutuantes para as inst\u00e2ncias.   <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip API_1 $FLOATING_IP\n</code></pre> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip API_2 $FLOATING_IP\n</code></pre> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip DATABASE $FLOATING_IP\n</code></pre> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip LOADBALANCER $FLOATING_IP\n</code></pre></li> <li> <p>Com isso, conseguimos acessar as inst\u00e2ncias via SSH, usando o IP flutuante e fazer as instala\u00e7\u00f5es necess\u00e1rias.</p> </li> <li> <p>Para ambas inst\u00e2ncias da API, utilizamos o docker para baixarmos a imagem do nosso projeto. J\u00e1 para a inst\u00e2ncia do banco de dados, tamb\u00e9m utilizamos o docker para baixarmos a imagem do MySQL. Por fim, na inst\u00e2ncia do LoadBalancer, instalamos o Nginx e configuramos o mesmo para fazer o balanceamento de carga entre as duas inst\u00e2ncias da API.</p> </li> <li> <p>Tanto na inst\u00e2ncia do banco de dados quanto nas inst\u00e2ncias da API, criamos um arquivo .env, com as vari\u00e1veis de ambiente <code>nano .env</code>, com as credenciais de acesso ao banco de dados, determinados no projeto.</p> </li> <li> <p>Com isso, para rodarmos as aplica\u00e7\u00f5es, utilizamos do comando <code>docker run</code> e tivemos que expor a porta 8080 com o comando <code>-p 8080:8080</code>, e tamb\u00e9m o comando <code>-env-file .env</code>, para que o docker conseguisse ler as vari\u00e1veis de ambiente do arquivo .env.   <pre><code>docker run -d -p 8080:8080 --env-file .env tanaka7/projetonuvem_ianvitor-api:latest\n</code></pre></p> </li> <li> <p>Por fim, fizemos um t\u00fanel SSH para o loadbalancer, com o comando <code>ssh cloud@10.103.1.26 -L 8080:172.16.7.8:80</code>, para que consegu\u00edssemos acessar a API via localhost:8080/docs.</p> </li> </ul>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":"<p> 1 - Print da aba network topology</p> <p> 2 - Print das inst\u00e2ncias no OpenStack</p> <p> 3 - Print do Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB</p> <p>     *4 - Print do Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB *</p> <p> 5 - Print da vis\u00e3o geral da inst\u00e2ncia DATABASE, mostrando em que server foi alocada</p> <p> 6 - Print da vis\u00e3o geral da inst\u00e2ncia LOADBALANCER, mostrando em que server foi alocada</p> <p> 7 - Print da vis\u00e3o geral da inst\u00e2ncia API_1, mostrando em que server foi alocada</p> <p> 8 - Print da vis\u00e3o geral da inst\u00e2ncia API_2, mostrando em que server foi alocada</p>"},{"location":"roteiro3/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Ao longo do roteiro, enfrentamos alguns desafios, principalmente que nosso server3 apresentava uma falha de hardware, em que possuia um disco a menos do necess\u00e1rio. Com isso, tivemos que adicionar o n\u00f3 reserva (node2) na parte de instala\u00e7\u00e3o antes da etapa de escalonamento de n\u00f3s, garantido a instala\u00e7\u00e3o de todos os servi\u00e7os necess\u00e1rios corretamente na infra. Com isso, tiramos o aprendizado de que \u00e9 importante verificar a sa\u00fade dos n\u00f3s mais frequentemente para evitar problemas, mas tamb\u00e9m tivemos a oportunidade de improvisar e testar solu\u00e7\u00f5es alternativas para lidar com imprevistos. </p>"},{"location":"roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Neste roteiro, conseguimos implementar uma nuvem privada com OpenStack e Juju, configurando os servi\u00e7os necess\u00e1rios para o funcionamento da nuvem. Al\u00e9m disso, conseguimos integrar a API do projeto com a nuvem, utilizando o LoadBalancer para balancear a carga entre as inst\u00e2ncias da API. Com isso, conseguimos entender melhor como funciona a infraestrutura de uma nuvem privada e como podemos utilizar o OpenStack e o Juju para gerenci\u00e1-la. </p>"},{"location":"roteiro3/mermaid/tarefa3/","title":"Tarefa3","text":"<pre><code>graph TD\n    A[Seu computador] -- 10.0.0.0/8 --&gt; B[Roteador Insper]\n    B -- 172.16.0.0/20 --&gt; C[OpenStack Controller main]\n    C -- T\u00fanel SSH --&gt; D[Load Balancer]\n    D -- 192.169.0.28 --&gt; E[API 1]\n    D -- 192.169.0.54 --&gt; F[API 2]\n    E -- 192.169.0.85 --&gt; G[Database]\n    F -- 192.169.0.85 --&gt; G[Database]</code></pre>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#roteiro-4-sla-dr-e-iac","title":"Roteiro 4: SLA, DR e IaC","text":""},{"location":"roteiro4/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este roteiro tem como objetivo utilizar a ferramenta de Infraestrutura como C\u00f3digo (IaC) Terraform para a cria\u00e7\u00e3o de um projeto, suas respectivas configura\u00e7\u00f5es e infraestrutura dentro da plataforma OpenStack. A proposta \u00e9 automatizar o provisionamento de recursos de forma padronizada e reprodut\u00edvel.</p>"},{"location":"roteiro4/main/#instalando-o-terraform","title":"Instalando o Terraform","text":"<p>O primeiro passo consiste na instala\u00e7\u00e3o do Terraform, utilizando os comandos listados abaixo:</p> <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\ngpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre>"},{"location":"roteiro4/main/#infra","title":"Infra","text":""},{"location":"roteiro4/main/#criando-a-hierarquia-de-projetos-separado-por-aluno","title":"Criando a hierarquia de projetos separado por aluno","text":"<p>O primeiro passo foi a cria\u00e7\u00e3o de um dom\u00ednio e dois projetos \u2014 um para cada integrante do grupo \u2014 e, em cada projeto, a cria\u00e7\u00e3o de um usu\u00e1rio denominado \"Aluno\", utilizando o Horizon Dashboard do OpenStack.</p>"},{"location":"roteiro4/main/#app","title":"App","text":""},{"location":"roteiro4/main/#criando-a-infraestrutura-utilizando-iac","title":"Criando a Infraestrutura utilizando IaC","text":"<p>Cada integante entrou na maquina main de seu computador pessoal para criar a sua estrutura. criando uma pasta do projeto e  uma do terraform dentro dele nessa estrutura: <pre><code>/meu-projeto/\n\u2514\u2500\u2500 terraform/\n    \u251c\u2500\u2500 provider.tf\n    \u251c\u2500\u2500 network.tf\n    \u251c\u2500\u2500 router.tf\n    \u251c\u2500\u2500 instance1.tf\n    \u2514\u2500\u2500 instance2.tf\n</code></pre></p> <ul> <li><code>provider.tf</code>: define o provedor e as credenciais do OpenStack.</li> <li><code>network.tf</code>: cria a rede e a sub-rede.</li> <li><code>router.tf</code>: configura o roteador e a interface de rede.</li> <li><code>instance1.tf</code> e <code>instance2.tf</code>: criam as inst\u00e2ncias de m\u00e1quina virtual.</li> </ul>"},{"location":"roteiro4/main/#providertf","title":"provider.tf","text":"<pre><code>terraform {\n  required_version = \"&gt;= 0.14.0\"\n  required_providers {\n    openstack = {\n      source  = \"terraform-provider-openstack/openstack\"\n      version = \"~&gt; 1.35.0\"\n    }\n  }\n}\n\nprovider \"openstack\" {\n  region    = \"RegionOne\"\n  user_name = \"Ian\"\n  password  = \"123\"\n  auth_url  = \"https://172.16.0.31:5000/v3\"\n  insecure  = true\n} \n</code></pre>"},{"location":"roteiro4/main/#networktf","title":"network.tf","text":"<pre><code>resource \"openstack_networking_network_v2\" \"network_1\" {\n  name           = \"network_1\"\n  admin_state_up = true\n}\n\nresource \"openstack_networking_subnet_v2\" \"subnet_1\" {\n  network_id = openstack_networking_network_v2.network_1.id\n  cidr       = \"192.167.199.0/24\"\n}\n</code></pre>"},{"location":"roteiro4/main/#routertf","title":"router.tf","text":"<pre><code>resource \"openstack_networking_router_v2\" \"router_1\" {\n  name                = \"my_router\"\n  admin_state_up      = true\n  external_network_id = \"ad2146a3-86bd-45bc-8bb7-3659c3af0fa9\"\n}\n\nresource \"openstack_networking_router_interface_v2\" \"int_1\" {\n  router_id = \"${openstack_networking_router_v2.router_1.id}\"\n  subnet_id = \"${openstack_networking_subnet_v2.subnet_1.id}\"\n}\n</code></pre>"},{"location":"roteiro4/main/#instance1tf","title":"instance1.tf","text":"<p><pre><code>resource \"openstack_compute_instance_v2\" \"instancia1_ian\" {\n  name            = \"instancia1_ian\"\n  image_id      = \"5e63750f-cbba-4ddb-986f-bab4971ada60\"\n  flavor_id     = \"db628d07-5bf0-407c-9f1f-4f49cceca6b6\"\n  key_pair        = \"chave_ian\"\n  security_groups = [\"default\"]\n\n\n\n  network {\n    name = \"network_1\"\n}\n\n}\n</code></pre> Ap\u00f3s isso, execute os seguintes comandos para criar o plano de execu\u00e7\u00e3o do Terraform e, em seguida, aplicar esse plano:</p> <pre><code>terraform plan    \nterraform apply   \n</code></pre>"},{"location":"roteiro4/main/#exercicios","title":"Exercicios","text":""},{"location":"roteiro4/main/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"roteiro4/main/#exercicio-3","title":"Exerc\u00edcio 3","text":"<p> Figura 2: Identy users no OpenStack.</p>"},{"location":"roteiro4/main/#exercicio-4","title":"Exerc\u00edcio 4","text":"<p> Figura 3: compute overview no OpenStack.</p> <p> Figura 4:ompute overview no OpenStack.</p>"},{"location":"roteiro4/main/#exercicio-5","title":"Exerc\u00edcio 5","text":"<p> Figura 5:compute instances no OpenStack</p> <p> Figura 6:compute instances no OpenStack</p>"},{"location":"roteiro4/main/#exercicio-6","title":"Exerc\u00edcio 6","text":"<p> Figura 7: network topology no OpenStack.</p> <p> Figura 8:network topology no OpenStack.</p>"},{"location":"roteiro4/main/#criando-um-plano-de-disaster-recovery-e-sla","title":"Criando um Plano de Disaster Recovery e SLA","text":""},{"location":"roteiro4/main/#cenario-voce-e-o-cto-de-uma-grande-empresa-com-sede-em-varias-capitais-no-brasil-e-precisa-implantar-um-sistema-critico-de-baixo-custo-e-com-dados-sigilosos-para-a-area-operacional","title":"Cen\u00e1rio: Voc\u00ea \u00e9 o CTO de uma grande empresa com sede em v\u00e1rias capitais no Brasil e precisa implantar um sistema cr\u00edtico, de baixo custo e com dados sigilosos para a \u00e1rea operacional.","text":"<p>Resposta: Optar\u00edamos por implementar um sistema em cloud p\u00fablica, pois inicialmente apresenta um custo mais baixo, j\u00e1 que n\u00e3o h\u00e1 necessidade de adquirir e manter servidores pr\u00f3prios. Al\u00e9m disso, provedores de cloud p\u00fablica j\u00e1 oferecem garantias de seguran\u00e7a, escalabilidade e disponibilidade, atendendo aos requisitos de prote\u00e7\u00e3o dos dados sigilosos e opera\u00e7\u00e3o eficiente.</p>"},{"location":"roteiro4/main/#explique-para-o-rh-por-que-voce-precisa-de-um-time-de-devops","title":"Explique para o RH por que voc\u00ea precisa de um time de DevOps.","text":"<p>Resposta: \u00c9 fundamental contar com um time de DevOps, pois esses profissionais garantem a integra\u00e7\u00e3o eficaz entre as \u00e1reas de desenvolvimento e opera\u00e7\u00f5es. Em ambientes de computa\u00e7\u00e3o em nuvem, a equipe de DevOps \u00e9 respons\u00e1vel pela cria\u00e7\u00e3o e manuten\u00e7\u00e3o da infraestrutura, al\u00e9m da gest\u00e3o dos ambientes de desenvolvimento, utilizando ferramentas modernas como IaC (Infraestrutura como C\u00f3digo) para garantir ambientes escal\u00e1veis, seguros e padronizados.</p> <p>Al\u00e9m disso, o time de DevOps atua na cria\u00e7\u00e3o de pipelines de integra\u00e7\u00e3o cont\u00ednua, organiza\u00e7\u00e3o e gerenciamento de containers, integra\u00e7\u00e3o eficiente de sistemas e monitoramento constante da infraestrutura, permitindo identificar e resolver falhas rapidamente, o que contribui diretamente para a agilidade e confiabilidade dos processos da empresa.</p>"},{"location":"roteiro4/main/#plano-de-continuidade-e-alta-disponibilidade-dr-e-ha","title":"Plano de Continuidade e Alta Disponibilidade (DR e HA)","text":""},{"location":"roteiro4/main/#mapeamento-de-ameacas","title":"Mapeamento de Amea\u00e7as","text":""},{"location":"roteiro4/main/#ameacas-fisicas","title":"Amea\u00e7as F\u00edsicas","text":"<ul> <li> <p>Falhas de hardware: Defeitos em componentes f\u00edsicos, como servidores ou discos r\u00edgidos, podem causar interrup\u00e7\u00f5es no servi\u00e7o, perda de dados e comprometimento da integridade do sistema.</p> </li> <li> <p>Desastres naturais: Eventos como inunda\u00e7\u00f5es, inc\u00eandios, terremotos, tsunamis ou erup\u00e7\u00f5es vulc\u00e2nicas podem danificar fisicamente os datacenters e equipamentos de rede e servidores, gerando indisponibilidade total da infraestrutura, perda de dados e falhas no fornecimento de energia.</p> </li> <li> <p>Acesso f\u00edsico n\u00e3o autorizado: Invas\u00f5es ou acesso indevido aos servidores podem resultar em danos f\u00edsicos, destrui\u00e7\u00e3o de equipamentos e roubo de dados confidenciais, al\u00e9m de abrir brechas para ataques futuros.</p> </li> </ul>"},{"location":"roteiro4/main/#ameacas-logicas","title":"Amea\u00e7as L\u00f3gicas","text":"<ul> <li>Ataques cibern\u00e9ticos: A\u00e7\u00f5es como DDoS, inje\u00e7\u00f5es de SQL, phishing e malwares representam riscos severos \u00e0 disponibilidade, integridade e confidencialidade do sistema e dos dados armazenados.</li> </ul>"},{"location":"roteiro4/main/#acoes-prioritarias-para-recuperacao-em-caso-de-desastres","title":"A\u00e7\u00f5es Priorit\u00e1rias para Recupera\u00e7\u00e3o em Caso de Desastres","text":"<ul> <li> <p>Backups de dados:   Realiza\u00e7\u00e3o de backups regulares e autom\u00e1ticos, armazenados em locais seguros, criptografados e geograficamente distantes (m\u00ednimo de 100 km) do datacenter principal, garantindo prote\u00e7\u00e3o contra desastres f\u00edsicos de grande escala.</p> </li> <li> <p>Balanceamento de carga (Load Balancer):   Implementa\u00e7\u00e3o de balanceadores de carga para distribuir o tr\u00e1fego de forma inteligente entre m\u00faltiplos servidores, prevenindo sobrecarga e mantendo o funcionamento dos servi\u00e7os mesmo em caso de falha de um dos n\u00f3s.</p> </li> <li> <p>Monitoramento cont\u00ednuo:   Implanta\u00e7\u00e3o de ferramentas de monitoramento proativo (f\u00edsico e virtual) que detectem falhas, anomalias de desempenho e poss\u00edveis ataques, possibilitando uma resposta r\u00e1pida e eficiente antes que o problema se agrave.</p> </li> <li> <p>Escalonamento \u00e0 disposi\u00e7\u00e3o:   Manter um plano de escalonamento e m\u00e1quinas reservas prontas para serem ativadas conforme a demanda. Dessa forma, \u00e9 poss\u00edvel aumentar rapidamente a capacidade da aplica\u00e7\u00e3o ou substituir n\u00f3s em caso de falha, garantindo a continuidade dos servi\u00e7os sem interrup\u00e7\u00f5es.</p> </li> </ul>"},{"location":"roteiro4/main/#politica-de-backup","title":"Pol\u00edtica de Backup","text":"<p>Iriamos realizar backups diarios em horarios predeterminados, em localiza\u00e7\u00f5es geograficamente com uma distancia minima de 100km dos servidores atuais</p>"},{"location":"roteiro4/main/#estrategia-de-alta-disponibilidade-ha","title":"Estrat\u00e9gia de Alta Disponibilidade (HA)","text":"<ul> <li>Redund\u00e2ncia geogr\u00e1fica:   Servi\u00e7os distribu\u00eddos em m\u00faltiplas zonas de disponibilidade, com replica\u00e7\u00e3o ativa/ativa ou ativa/passiva de componentes cr\u00edticos.</li> <li>Load Balancer:   Utiliza\u00e7\u00e3o de balanceadores de carga para distribuir o tr\u00e1fego entre diferentes servidores, garantindo que a aplica\u00e7\u00e3o suporte um volume maior de acessos e seja facilmente escalon\u00e1vel, al\u00e9m de aumentar a disponibilidade e toler\u00e2ncia a falhas.</li> <li>Queues:   Caso a demanda ultrapasse a capacidade de processamento, o sistema deve ser capaz de colocar as requisi\u00e7\u00f5es em uma fila, garantindo que todas sejam atendidas assim que houver recursos dispon\u00edveis. Isso evita sobrecargas, mant\u00e9m a estabilidade do sistema e assegura o atendimento de todas as solicita\u00e7\u00f5es mesmo durante picos de acesso.</li> </ul>"}]}