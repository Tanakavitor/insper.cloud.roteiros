{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-q","title":"KIT-Q","text":"<p>Ian Faray </p> <p>Vitor Tanaka</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 17/03/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1 - Computa\u00e7\u00e3o em Nuvem","text":"<p>Membros: Ian Faray e Vitor Tanaka</p>"},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Este roteiro tem como objetivo demonstrar o processo de cria\u00e7\u00e3o de uma infraestrutura de Nuvem Bare-metal, em que introduziram-se conceitos b\u00e1sicos sobre gerenciamento de hardware e rede de computadores. Utilizamos um ambiente composto por cinco m\u00e1quinas, onde implementamos:</p> <ul> <li> <p>Um servidor PostgreSQL para gerenciamento de banco de dados.</p> </li> <li> <p>Duas inst\u00e2ncias de uma aplica\u00e7\u00e3o Django, distribu\u00eddas em servidores distintos.</p> </li> <li> <p>Um servidor Nginx configurado como balanceador de carga, distribuindo as requisi\u00e7\u00f5es entre as aplica\u00e7\u00f5es Django.</p> </li> </ul>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":""},{"location":"roteiro1/main/#infraestrutura","title":"Infraestrutura","text":"<p>Inicialmente, criamos a infraestrutura de Nuvem Bare-metal:</p> <ul> <li> <p>Instalamos o Ubuntu Server 22.04 LTS na NUC principal, sendo nossa main, utilizando um pendrive boot\u00e1vel.</p> </li> <li> <p>Configuramos o MAAS para gerenciar as m\u00e1quinas, sendo acessado via autentica\u00e7\u00e3o SSH.</p> </li> <li> <p>Adicionamos as 5 m\u00e1quinas dispon\u00edveis (server1 a server5) ao MAAS, configurando-as para inicializa\u00e7\u00e3o via PXE.</p> </li> <li> <p>Criamos uma OVS Bridge para conectar as interfaces de rede, garantindo a comunica\u00e7\u00e3o entre as m\u00e1quinas.</p> </li> <li> <p>Realizamos um NAT para permitir o acesso externo a partir da rede Insper, conectando ao server main pela porta 22.</p> </li> </ul>"},{"location":"roteiro1/main/#aplicacao","title":"Aplica\u00e7\u00e3o","text":"<p>Primeiramente, realizamos o deploy do Ubuntu no server1 pelo MAAS, em que, em seguida, instalamos o PostgreSQL.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p> Tarefa 1.1 Print Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional. </p> <p> Tarefa 1.2 Acessivel na pr\u00f3pria maquina na qual ele foi implantado. Tarefa 1.3 Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN</p> <p> Tarefa 1.4 Em qual porta este servi\u00e7o est\u00e1 funcionando</p>"},{"location":"roteiro1/main/#parte-ii-aplicacao-django","title":"Parte II: Aplica\u00e7\u00e3o Django","text":"<p>No segundo servidor, assim como no primeiro, realizamos o deploy do Ubuntu, por\u00e9m neste caso instalamos uma aplica\u00e7\u00e3o Django j\u00e1 desenvolvida.   </p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p> Tarefa 2.1 Dashboard do MAAS com as m\u00e1quinas</p> <p> Tarefa 2.2 aba images, com as imagens sincronizadas.</p> <p> Tarefa 2.3-1</p> <p> Tarefa 2.3-2</p> <p> Tarefa 2.3-3</p> <p> Tarefa 2.3-4</p> <p> Tarefa 2.3-5 Da Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK</p>"},{"location":"roteiro1/main/#utilizando-o-ansible-deploy-automatizado-de-aplicacao","title":"Utilizando o Ansible - deploy automatizado de aplica\u00e7\u00e3o","text":""},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<p> Tarefa 3.1  print da tela do Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs.</p> <p> Tarefa 3.2 print da aplicacao Django, provando que voce est\u00e1 conectado ao server</p> <p>tarefa 3.3:  A implementa\u00e7\u00e3o manual do banco de dados no servidor 1 foi realizada via terminal, come\u00e7ando com a instala\u00e7\u00e3o do PostgreSQL. Em seguida, foi criado um superusu\u00e1rio e uma base de dados, al\u00e9m de configurar o banco para aceitar conex\u00f5es de qualquer m\u00e1quina dentro da nossa sub-rede. Para a aplica\u00e7\u00e3o Django no servidor 2, clonamos um reposit\u00f3rio que j\u00e1 continha o c\u00f3digo da aplica\u00e7\u00e3o e utilizamos um script (install.sh). Esse script instalava automaticamente todas as depend\u00eancias necess\u00e1rias, realizava a migra\u00e7\u00e3o do banco de dados e configurava outro script para garantir que o servidor fosse iniciado automaticamente ap\u00f3s um reboot. A aplica\u00e7\u00e3o foi configurada para rodar na porta 8080, e tamb\u00e9m foi criado um superusu\u00e1rio para acesso ao Django Admin. Para conseguirmos testar a aplica\u00e7\u00e3o Django rodando no nosso computador, configuramos um t\u00fanel SSH redirecionando a porta 8080 do servidor remoto para a porta 8001 local.</p>"},{"location":"roteiro1/main/#ansible","title":"Ansible","text":"<p>Utilizamos o Ansible para fazermos deploy da aplica\u00e7\u00e3o Django. No nosso caso, escolhemos o server 4.</p>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<p> Tarefa 4.1 print da tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs.</p> <p> *Tarefa 4.2 print da aplicacao Django, provando que voce est\u00e1 conectado ao server2 *</p> <p> Tarefa 4.3 print da aplicacao Django, provando que voce est\u00e1 conectado ao server3(4) no nosso caso</p> <p>Tarefa 4.4:A grande diferen\u00e7a \u00e9 que o Ansible permite automatizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o do Django, enquanto a instala\u00e7\u00e3o manual exige que cada etapa seja realizada individualmente. Com o Ansible, utilizamos um playbook que cont\u00e9m todos os scripts necess\u00e1rios para definir e executar as instru\u00e7\u00f5es de instala\u00e7\u00e3o e configura\u00e7\u00e3o do Django. Na instala\u00e7\u00e3o manual, \u00e9 preciso realizar cada passo manualmente, como clonar o reposit\u00f3rio e ajustar configura\u00e7\u00f5es, o que pode ser mais demorado e propenso a erros. J\u00e1 com o Ansible, h\u00e1 um script padronizado que n\u00e3o s\u00f3 agiliza o processo, mas tamb\u00e9m facilita para escalonar o projeto caso necess\u00e1rio</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<p> Tarefa 5.1 print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.</p> <p> Tarefa 5.2-1</p> <p> Tarefa 5.2-2prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3 (no nosso caso, server4).</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Apesar da pr\u00e1tica ter sido um sucesso, encontramos diversos obst\u00e1culos e dificuldades no processo at\u00e9 atingirmos o resultado final. O primeiro contato com o MAAS e a configura\u00e7\u00e3o da infraestrutura foi desafiador, sendo necess\u00e1rio pesquisar e consultar os professores e assistentes, que nos orientaram e auxiliaram para superar as dificuldades. Al\u00e9m disso, imprevistos ocorreram, at\u00e9 inesperados pelos professores, como por exemplo, as imagens do Ubuntu que instalamos no MAAS sofreram uma atualiza\u00e7\u00e3o e ficaram incompat\u00edveis com o firmware das m\u00e1quinas, o que nos obrigou a voltar alguns passos para continuar o roteiro. Apesar disso, esses incidentes s\u00e3o importantes para aprendermos a lidar com problemas reais e a buscar solu\u00e7\u00f5es em grupo, o que contribui para o nosso desenvolvimento profissional e conhecimento t\u00e9cnico.</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro permitiu consolidar conhecimentos sobre redes, sub-redes e gerenciamento de infraestrutura em nuvem. Al\u00e9m disso, exploramos ferramentas como MAAS, Ansible e Nginx, que facilitam a implementa\u00e7\u00e3o e o gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas. O balanceamento de carga e a alta disponibilidade foram alcan\u00e7ados com sucesso, demonstrando a efic\u00e1cia da infraestrutura criada.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2: Utilizando a Infraestrutura Bare Metal com o Juju","text":""},{"location":"roteiro2/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este roteiro tem como objetivo demonstrar o uso do Juju como orquestrador de aplica\u00e7\u00f5es distribu\u00eddas, integrado ao MAAS para provisionamento autom\u00e1tico de m\u00e1quinas bare-metal. Como aplica\u00e7\u00e3o de exemplo, implementamos o Grafana e o Prometheus, dois servi\u00e7os populares para monitoramento e visualiza\u00e7\u00e3o de dados.</p>"},{"location":"roteiro2/main/#configuracao-do-ambiente","title":"Configura\u00e7\u00e3o do Ambiente","text":""},{"location":"roteiro2/main/#instalacao-do-juju","title":"Instala\u00e7\u00e3o do Juju","text":"<ul> <li> <p>Instalamos o Juju na m\u00e1quina main.</p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre> </li> </ul>"},{"location":"roteiro2/main/#integracao-com-o-maas","title":"Integra\u00e7\u00e3o com o MAAS","text":"<ul> <li> <p>Criamos um arquivo de configura\u00e7\u00e3o (maas-cloud.yaml) para adicionar o cluster MAAS ao Juju e um arquivo de credenciais (maas-creds.yaml) para autentica\u00e7\u00e3o.</p> <p><pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre></p> </li> </ul>"},{"location":"roteiro2/main/#criacao-do-controlador","title":"Cria\u00e7\u00e3o do Controlador","text":"<ul> <li>Criamos o controlador Juju no servidor server1 utilizando a tag juju.  </li> <li>O controlador foi configurado para gerenciar toda a infraestrutura.</li> </ul> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre>"},{"location":"roteiro2/main/#deploy-das-aplicacoes","title":"Deploy das Aplica\u00e7\u00f5es","text":""},{"location":"roteiro2/main/#instalacao-do-dashboard-do-juju","title":"Instala\u00e7\u00e3o do Dashboard do Juju","text":"<ul> <li> <p>Instalamos o Dashboard do Juju </p> <p><pre><code>$ juju switch controller\n</code></pre> <pre><code>$ juju deploy juju-dashboard dashboard\n</code></pre> <pre><code>$ juju integrate dashboard controller\n</code></pre> <pre><code>$ juju expose dashboard\n</code></pre> <pre><code>$ juju dashboard\n</code></pre></p> </li> </ul>"},{"location":"roteiro2/main/#deploy-do-grafana-e-prometheus","title":"Deploy do Grafana e Prometheus","text":"<ul> <li> <p>Instalamos o Grafana e Prometheus e realizamos o deploy local dos charms respectivos utilizando o Juju.  </p> <p><pre><code>$ mkdir -p /home/cloud/charms\n</code></pre> <pre><code>$ cd /home/cloud/charms\n</code></pre> <pre><code>$ juju download grafana\n</code></pre> <pre><code>$ juju download prometheus2\n</code></pre> - Deploy dos charms: <pre><code>$ juju deploy (grafana e prometheus)\n</code></pre></p> </li> </ul>"},{"location":"roteiro2/main/#integracao-entre-grafana-e-prometheus","title":"Integra\u00e7\u00e3o entre Grafana e Prometheus","text":"<ul> <li> <p>Integramos o Grafana com o Prometheus.  </p> <pre><code>$ juju integrate grafana prometheus2\n</code></pre> </li> </ul>"},{"location":"roteiro2/main/#atividades","title":"Atividades","text":"<p> 1 - Print da tela do Dashboard do MAAS com as Maquinas e seus respectivos IPs </p> <p> 2 - Print de tela do comando \"juju status\" depois que o Grafana estiver \"active\"</p> <p> 3 - Print da tela do Dashboard do Grafana com o Prometheus aparecendo como source</p> <p> 4 - Print que voc\u00ea est\u00e1 conseguindo acessar o Dashboard a partir da rede do Insper</p> <p> 5 - Print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU</p>"},{"location":"roteiro2/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Apesar de curto, encontramos algumas dificuldades ao longo da realiza\u00e7\u00e3o do roteiro. Como foi um primeiro contato com o Juju, \u00e9 natural que tenha sido um desafio a instala\u00e7\u00e3o e configura\u00e7\u00e3o correta do mesmo, envolvendo leituras de documenta\u00e7\u00f5es e entendimento de seu funcionamento e prop\u00f3sito. Por exemplo, ao realizarmos o deploy do charm do Grafana, tivemos que for\u00e7ar um versionamento correto do Ubuntu (20.04), pois a vers\u00e3o instalada pelo padr\u00e3o do Juju (22.04) n\u00e3o era compat\u00edvel com o charm. Os problemas enfrentados geram um engajamento maior com o aprendizado da ferramenta e nos obrigam a buscar entender o funcionamento do Juju e os conceitos por tr\u00e1s dele.</p>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro nos proporcionou um aprendizado significativo sobre o uso do Juju como orquestrador de aplica\u00e7\u00f5es distribu\u00eddas, al\u00e9m de nos familiarizar ainda mais com o MAAS para provisionamento autom\u00e1tico de m\u00e1quinas bare-metal. A integra\u00e7\u00e3o entre o Grafana e o Prometheus demonstrou a efic\u00e1cia do Juju na gest\u00e3o de servi\u00e7os complexos, facilitando a implementa\u00e7\u00e3o e o monitoramento de aplica\u00e7\u00f5es em ambientes bare-metal. A experi\u00eancia adquirida neste roteiro ser\u00e1 valiosa para futuras implementa\u00e7\u00f5es e projetos que envolvam orquestra\u00e7\u00e3o e automa\u00e7\u00e3o de infraestrutura.</p>"},{"location":"roteiro3/main/","title":"Roteiro 3: Private cloud com Openstack e Juju","text":""},{"location":"roteiro3/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este roteiro tem como objetivo a implementa\u00e7\u00e3o de um ambiente de nuvem privada, entendendo os conceitos b\u00e1sicos e aplicando na pr\u00e1tica com o uso do OpenStack e do Juju. O OpenStack \u00e9 uma plataforma de computa\u00e7\u00e3o em nuvem de c\u00f3digo aberto que permite a cria\u00e7\u00e3o e gerenciamento de infraestrutura como servi\u00e7o (IaaS). O Juju, j\u00e1 utilizado anteriormente, \u00e9 uma ferramenta de orquestra\u00e7\u00e3o que facilita o gerenciamento de servi\u00e7os e aplica\u00e7\u00f5es em ambientes distribu\u00eddos.</p>"},{"location":"roteiro3/main/#configuracao-do-ambiente-infra","title":"Configura\u00e7\u00e3o do Ambiente - Infra","text":""},{"location":"roteiro3/main/#implantacao-do-openstack","title":"Implanta\u00e7\u00e3o do OpenStack","text":"<ul> <li> <p>Primeiramente, no MAAS, adicionamos as seguintes tags \u00e0s m\u00e1quinas:</p> <ul> <li>controller - node1</li> <li>reserva - node2 (ser\u00e1 usado posteriormente)</li> <li>compute - node3, node4 e node5</li> </ul> </li> <li> <p>Com isso, verificamos no MAAS se o br-ex estava configurado corretamente em todos os n\u00f3s, para assim, o OpenStack conseguir se comunicar com a rede externa.</p> </li> <li> <p>Ap\u00f3s as configura\u00e7\u00f5es iniciais, implementamos o deploy do juju controller no node1, para fazermos as instala\u00e7\u00f5es necess\u00e1rias em todos os n\u00f3s.</p> </li> <li> <p>Por fim, com os comandos juju deploy e juju integrate e com arquivos de configura\u00e7\u00e3o .yaml, instalamos todos os programas requisitados no roteiro, sempre verificando pelo juju status as m\u00e1quinas e instala\u00e7\u00f5es.</p> <ul> <li>Ceph OSD</li> <li>Nova Compute</li> <li>MySQL InnoDB Cluster</li> <li>Vault</li> <li>Neutron Networking</li> <li>Keystone</li> <li>RabbitMQ</li> <li>Nova Cloud Controller</li> <li>Placement</li> <li>Horizon - OpenStack Dashboard</li> <li>Glance</li> <li>Ceph Monitor</li> <li>Cinder</li> <li>Ceph RADOS Gateway</li> <li>Ceph-OSD Integration para as m\u00e1quinas e aplica\u00e7\u00f5es estarem active</li> </ul> </li> </ul>"},{"location":"roteiro3/main/#configurando-o-openstack-setup","title":"Configurando o OpenStack - Setup","text":"<ul> <li>Pr\u00f3ximo passo foi configurar o OpenStack, configurando os servi\u00e7os que controlam VMs (Nova), os volumes de disco (Cinder) e a estrutura de rede virtual (Neutron). Para isso, a realiza\u00e7\u00e3o foi separada nos seguintes passos: </li> </ul>"},{"location":"roteiro3/main/#passo-1-autenticacao","title":"Passo 1 - Autentica\u00e7\u00e3o","text":"<ul> <li>Na main, criamos o arquivo openrc, que cont\u00e9m as credenciais de acesso ao OpenStack.</li> </ul> <p><pre><code> source openrc \n</code></pre> <pre><code>env | grep OS_\n</code></pre>  - Com isso, conseguimos o usu\u00e1rio e senha para acessar o Horizon, o Dashboard do OpenStack.</p>"},{"location":"roteiro3/main/#passo-2-horizon","title":"Passo 2 - Horizon","text":"<ul> <li>Com as credenciais prontas, acessamos o Dashboard do Openstack, chamado de Horizon. Para isso, realizamos um NAT no roteador para o Horizon na porta interna 80 e porta externa 8080, acessando a partir do 10.103.1.26:8080/horizon. O Domain name utilizado no acesso foi admin_domain.</li> </ul>"},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":"<p> 1 - Print do status do Juju </p> <p> 2 - Print do Dashboard do MAAS com as m\u00e1quinas</p> <p> 3 - Print da aba compute overview no OpenStack Dashboard</p> <p> 4 - Print da aba compute instances no OpenStack Dashboard</p> <p> 5 - Print da aba network topology no Openstack Dashboard</p>"},{"location":"roteiro3/main/#passo-3-imagens-e-flavors","title":"Passo 3 - Imagens e Flavors","text":"<ul> <li> <p>Utilizando a documenta\u00e7\u00e3o oficial do OpenStack, nessa etapa criamos imagens e flavors para nossa nuvem. Imagem se refere ao sistema operacional a ser bootado nas m\u00e1quinas virtuais a serem implementadas. J\u00e1 os flavors determinam os specs de hardware da VM, por exemplo: mem\u00f3ria RAM, armazenamento, CPU.</p> </li> <li> <p>Para isso, primeiramente instalamos o client do OpenStack na main.  <pre><code>sudo snap install openstackclients\n</code></pre></p> </li> <li>Em seguida, carregamos as credenciais, verificamos os servi\u00e7os dispon\u00edveis e fizemos pequenos ajustes na rede.  <pre><code>source openrc\n</code></pre> <pre><code>openstack service list\n</code></pre> <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.16.0.1\"\n</code></pre></li> <li>Logo ap\u00f3s, importamos a imagem do Ubuntu Jammy e a chamamos de jammy-amd64  <pre><code>mkdir ~/cloud-images\n\nwget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\\n  -O ~/cloud-images/jammy-amd64.img\n</code></pre> <pre><code>openstack image create --public --container-format bare \\\n  --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\\n  jammy-amd64\n</code></pre></li> <li>Por fim, criamos 4 flavors:<ul> <li><code>m1.tiny: 1 vCPUs, 1Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 1024 --disk 20 --vcpus 1 m1.tiny\n</code></pre></li> <li><code>m1.small: 1 vCPUs, 2Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 2048 --disk 20 --vcpus 1 m1.small\n</code></pre></li> <li><code>m1.medium: 2 vCPUs, 4Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 4096 --disk 20 --vcpus 2 m1.medium\n</code></pre></li> <li><code>m1.large: 4 vCPUs, 8Gb de RAM, 20Gb de disco</code> <pre><code>openstack flavor create --ram 8192 --disk 20 --vcpus 4 m1.large\n</code></pre></li> </ul> </li> </ul>"},{"location":"roteiro3/main/#passo-4-rede-externa","title":"Passo 4 - Rede Externa","text":"<ul> <li>Nessa etapa, configuramos a rede externa da nuvem, com uma faixa entre <code>172.16.7.0</code> e <code>172.16.8.255</code> <pre><code>openstack network create --external --share \\\n  --provider-network-type flat --provider-physical-network physnet1 \\\n  ext_net\n</code></pre></li> <li>Subnet  <pre><code>openstack subnet create --network ext_net --no-dhcp \\\n  --gateway 172.16.0.1 --subnet-range 172.16.0.0/2 \\\n  --allocation-pool start=172.16.7.0,end=172.16.8.255 \\\n  ext_subnet\n</code></pre></li> </ul> <p>### Passo 5 - Rede Interna  e  Roteador  - Nessa etapa, configuramos a rede interna da nuvem, com a subnet <code>192.169.0.0/24</code>, e o roteador.  <pre><code>openstack network create --internal user1_net\n\nopenstack subnet create --network user1_net \\\n  --subnet-range 192.169.0.0/24 \\\n  --allocation-pool start=192.169.0.10,end=192.169.0.90 \\\n  user1_subnet\n</code></pre> <pre><code>openstack router create user1_router\nopenstack router add subnet user1_router user1_subnet\nopenstack router set user1_router --external-gateway ext_net\n</code></pre>  ### Passo 6 - Conex\u00e3o  - Primeiramente, importamos a chave p\u00fablica j\u00e1 gerada anteriormente para o MAAS, para que possamos acessar as VMs via SSH.   <pre><code>openstack keypair create --public-key ~/.ssh/id_rsa.pub user1\n</code></pre>  - Em seguida, usando o Horizon, adicionamos a libera\u00e7\u00e3o do SSH e ALL ICMP no Security Group, para que possamos acessar as VMs e fazer ping. </p> <p>### Passo 7 - Inst\u00e2ncia  - Com tudo configurado, criamos uma inst\u00e2ncia com o flavor m1.tiny.   <pre><code>openstack server create --flavor m1.tiny --image jammy-amd64 \\\n  --key-name user1 --network user1_net --security-group default \\\n  jammy-1\n</code></pre>   - Ap\u00f3s isso, alocamos um floating IP para a inst\u00e2ncia.   <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip jammy-1 $FLOATING_IP\n</code></pre>   - Com isso, conseguimos acessar a inst\u00e2ncia via SSH, usando o IP flutuante.   <pre><code>ssh ubuntu@&lt;floating_ip&gt;\n</code></pre></p> <p>#### Tarefa 2    1 - Print do Dashboard do MAAS com as m\u00e1quinas</p> <p> 2 - Print da aba compute overview no OpenStack Dashboard</p> <p> 3 - Print da aba compute instances no OpenStack Dashboard</p> <p> 4 - Print da aba network topology no Openstack Dashboard</p> <ul> <li>As diferen\u00e7as encontradas entre os prints da tarefa 1 e tarefa 2 s\u00e3o simplesmente o resultado da execu\u00e7\u00e3o dos comandos de configura\u00e7\u00e3o do OpenStack. O dashboard do MAAS n\u00e3o mudou, j\u00e1 que tivemos problemas com o server3 e, portanto, adicionamos o n\u00f3 reserva (node2) na parte de instala\u00e7\u00e3o antes da etapa de escalonamento de n\u00f3s. A aba compute overview nos mostra uma vis\u00e3o geral, com todos os recursos adicionados no setup. J\u00e1 a aba compute instances mostra a inst\u00e2ncia jammy1 criada. Por fim, a aba network topology mostra a rede interna e externa criadas, com o roteador e as subnets, todas criadas no setup.</li> <li>Todos os recursos foram criados pela CLI do OpenStack, em que instalamos o openstackclient no come\u00e7o da etapa.</li> </ul> <p>### Escalando os n\u00f3s   - Como dito anteriormente, o server3 apresentou problemas, ent\u00e3o realizamos esta etapa antes do setup com os seguintes comandos:   <pre><code>juju add-unit nova-compute\n</code></pre> <pre><code>juju add-unit --to &lt;id do node2&gt; ceph-osd\n</code></pre>  #### Tarefa 3  <code>mermaid graph TD     A[Seu computador] -- 10.0.0.0/8 --&gt; B[Roteador Insper]     B -- 172.16.0.0/20 --&gt; C[OpenStack Controller main]     C -- T\u00fanel SSH --&gt; D[Load Balancer]     D -- 192.169.0.28 --&gt; E[API 1]     D -- 192.169.0.54 --&gt; F[API 2]     E -- 192.169.0.85 --&gt; G[Database]     F -- 192.169.0.85 --&gt; G[Database]</code></p>"},{"location":"roteiro3/main/#app","title":"App","text":""},{"location":"roteiro3/main/#uso-da-infraestrutura","title":"Uso da infraestrutura","text":"<ul> <li> <p>Ultima etapa do roteiro foi integrar a API criada na primeira etapa do projeto da disciplina com a nuvem privada criada. </p> </li> <li> <p>Para isso, foi necess\u00e1rio levantar as aplica\u00e7\u00f5es em 4 inst\u00e2ncias do OpenStack:</p> <ul> <li>2 inst\u00e2ncias com a API do projeto, implementada em FastAPI - m1.tiny</li> <li>1 inst\u00e2ncia com o banco de dados, implementado em MySQL - m1.small</li> <li>1 inst\u00e2ncia com LoadBalancer, implementado em Nginx - m1.tiny</li> </ul> <p></p> </li> <li> <p>Para isso, utilizamos o mesmo procedimento da etapa anterior, criando as inst\u00e2ncias com os flavors m1.tiny e m1.small alocando os IPs flutuantes.   <code>openstack server create --flavor m1.tiny --image jammy-amd64 \\     --key-name user1 --network user1_net --security-group default \\     API_1</code> <code>openstack server create --flavor m1.tiny --image jammy-amd64 \\     --key-name user1 --network user1_net --security-group default \\     API_2</code> <code>openstack server create --flavor m1.small --image jammy-amd64 \\     --key-name user1 --network user1_net --security-group default \\     DATABASE</code> <code>openstack server create --flavor m1.tiny --image jammy-amd64 \\     --key-name user1 --network user1_net --security-group default \\     LOADBALANCER</code></p> </li> <li>Ap\u00f3s isso, alocamos os IPs flutuantes para as inst\u00e2ncias.   <code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)   openstack server add floating ip API_1 $FLOATING_IP</code> <code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)   openstack server add floating ip API_2 $FLOATING_IP</code> <code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)   openstack server add floating ip DATABASE $FLOATING_IP</code> <code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)   openstack server add floating ip LOADBALANCER $FLOATING_IP</code></li> <li> <p>Com isso, conseguimos acessar as inst\u00e2ncias via SSH, usando o IP flutuante e fazer as instala\u00e7\u00f5es necess\u00e1rias.</p> </li> <li> <p>Para ambas inst\u00e2ncias da API, utilizamos o docker para baixarmos a imagem do nosso projeto. J\u00e1 para a inst\u00e2ncia do banco de dados, tamb\u00e9m utilizamos o docker para baixarmos a imagem do MySQL. Por fim, na inst\u00e2ncia do LoadBalancer, instalamos o Nginx e configuramos o mesmo para fazer o balanceamento de carga entre as duas inst\u00e2ncias da API.</p> </li> <li> <p>Tanto na inst\u00e2ncia do banco de dados quanto nas inst\u00e2ncias da API, criamos um arquivo .env, com as vari\u00e1veis de ambiente <code>nano .env</code>, com as credenciais de acesso ao banco de dados, determinados no projeto.</p> </li> <li> <p>Com isso, para rodarmos as aplica\u00e7\u00f5es, utilizamos do comando <code>docker run</code> e tivemos que expor a porta 8080 com o comando <code>-p 8080:8080</code>, e tamb\u00e9m o comando <code>-env-file .env</code>, para que o docker conseguisse ler as vari\u00e1veis de ambiente do arquivo .env.   <code>docker run -d -p 8080:8080 --env-file .env tanaka7/projetonuvem_ianvitor-api:latest</code></p> </li> <li> <p>Por fim, fizemos um t\u00fanel SSH para o loadbalancer, com o comando <code>ssh cloud@10.103.1.26 -L 8080:172.16.7.8:80</code>, para que consegu\u00edssemos acessar a API via localhost:8080/docs.</p> </li> </ul>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":"<p> 1 - Print da aba network topology</p> <p> 2 - Print das inst\u00e2ncias no OpenStack</p> <p> 3 - Print do Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB</p> <p>     *4 - Print do Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB *</p> <p> 5 - Print da vis\u00e3o geral da inst\u00e2ncia DATABASE, mostrando em que server foi alocada</p> <p> 6 - Print da vis\u00e3o geral da inst\u00e2ncia LOADBALANCER, mostrando em que server foi alocada</p> <p> 7 - Print da vis\u00e3o geral da inst\u00e2ncia API_1, mostrando em que server foi alocada</p> <p> 8 - Print da vis\u00e3o geral da inst\u00e2ncia API_2, mostrando em que server foi alocada</p>"},{"location":"roteiro3/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Ao longo do roteiro, enfrentamos alguns desafios, principalmente que nosso server3 apresentava uma falha de hardware, em que possuia um disco a menos do necess\u00e1rio. Com isso, tivemos que adicionar o n\u00f3 reserva (node2) na parte de instala\u00e7\u00e3o antes da etapa de escalonamento de n\u00f3s, garantido a instala\u00e7\u00e3o de todos os servi\u00e7os necess\u00e1rios corretamente na infra. Com isso, tiramos o aprendizado de que \u00e9 importante verificar a sa\u00fade dos n\u00f3s mais frequentemente para evitar problemas, mas tamb\u00e9m tivemos a oportunidade de improvisar e testar solu\u00e7\u00f5es alternativas para lidar com imprevistos. </p>"},{"location":"roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Neste roteiro, conseguimos implementar uma nuvem privada com OpenStack e Juju, configurando os servi\u00e7os necess\u00e1rios para o funcionamento da nuvem. Al\u00e9m disso, conseguimos integrar a API do projeto com a nuvem, utilizando o LoadBalancer para balancear a carga entre as inst\u00e2ncias da API. Com isso, conseguimos entender melhor como funciona a infraestrutura de uma nuvem privada e como podemos utilizar o OpenStack e o Juju para gerenci\u00e1-la. </p>"},{"location":"roteiro3/mermaid/tarefa3/","title":"Tarefa3","text":"<pre><code>graph TD\n    A[Seu computador] -- 10.0.0.0/8 --&gt; B[Roteador Insper]\n    B -- 172.16.0.0/20 --&gt; C[OpenStack Controller main]\n    C -- T\u00fanel SSH --&gt; D[Load Balancer]\n    D -- 192.169.0.28 --&gt; E[API 1]\n    D -- 192.169.0.54 --&gt; F[API 2]\n    E -- 192.169.0.85 --&gt; G[Database]\n    F -- 192.169.0.85 --&gt; G[Database]</code></pre>"}]}